---
title: "Predictive Analytics Exam Module 6, Section 5"

---
If you are using R 3.6.0 or later, the following command will ensure that the random number generator being used is the same as in prior versions of R. Because these modules were developed using R 3.5.0, output that depends on random numbers used the older generator. Running this code will have your output match that shown in the modules.


```{r}
# Run this chunk if using R 3.6.0 or later.
RNGkind(sample.kind = "Rounding")
```

If you are using R 4.0.0 or later, the following command will ensure that read.csv() interprets variables whose values are characters as factor variables. This was the default behavior in prior versions of R. All code is written assuming such variables are factor variables.

```{r}
# Run this chunk if using R 4.0.0 or later. You may get what looks like an error message. It can be ignored.
options(stringsAsFactors = TRUE)
```

Run CHUNK 1 to load the AutoClaim data set and examine the two target variables.

```{r}
# CHUNK 1
AutoClaim <- read.csv("AutoClaim.csv")
AutoClaim$CLM_FLAG_NUM <- ifelse(AutoClaim$CLM_FLAG == "Yes", 1, 0)
summary(AutoClaim$CLM_FLAG_NUM)
summary(AutoClaim$CLM_AMT5)
```

CHUNK 2 sets up initial GLMs for the two variables. 

```{r Answer - Initial GLMs}
# CHUNK 2

glm.freq <- glm(
  formula = CLM_FLAG_NUM ~ AGE + GENDER + MARRIED + JOBCLASS + MAX_EDUC + BLUEBOOK,
  data = AutoClaim,
  family = binomial(link = "logit")
)

glm.sev <- glm(
  formula = CLM_AMT5 ~ AGE + GENDER + MARRIED + CAR_USE + BLUEBOOK + CAR_TYPE + AREA,
  # Keeping only observations with positive claim amounts
  data = AutoClaim[which(AutoClaim$CLM_AMT5 > 0), ],
  family = zzz(link = "log")
) # zzz = Gamma or inverse.gaussian, Select one to be your model.
```

Run CHUNK 3 to see the summary for the fitted frequency model.

```{r}
# CHUNK 3
summary(glm.freq)
```

Run CHUNK 4 to see the exponentiated coefficients. These approximate the multiplicative effect of a unit change in the variable.
  
```{r}
# CHUNK 4
exp(coef(glm.freq))
```
  
Run CHUNK 5 to obtain confidence intervals for the exponentiated coefficients.

```{r}
# CHUNK 5
exp(confint.default(glm.freq))
```

Run CHUNK 6 to examine a reduced model.

```{r}
# CHUNK 6
# Removed MAX_EDUC
glm.freq2 <- glm(
  formula = CLM_FLAG_NUM ~ AGE + GENDER + MARRIED + JOBCLASS + BLUEBOOK,
  data = AutoClaim,
  family = binomial(link = "logit")
)

# anova(object, ..., dispersion = NULL, test = NULL)
# test - "Chisq", "LRT", "Rao", "F" or "Cp"
anova(glm.freq, glm.freq2, test = NULL)
```

Run CHUNK 7 to use AIC.

```{r}
# CHUNK 7
AIC(glm.freq, glm.freq2)
drop1(glm.freq) # AIC is the default
```

Run CHUNK 8 to see some residual plots. 

```{r}
# CHUNK 8
library(ggplot2)
library(gridExtra)
p1 <- qplot(x = glm.sev$fitted.values, y = residuals(glm.sev))
p2 <- qplot(x = glm.freq$fitted.values, y = residuals(glm.freq))
grid.arrange(p1, p2, ncol = 2)
```

Chunks 9 and 10 provide a function to make "crunch residuals" and then display them for the current frequency model.

```{r}
# CHUNK 9
# Mean residual (actual - predicted)
library(data.table)
crunch_residual <- function(dataset, model, target, size = 50) {
  x <- dataset[, target]
  x1 <- model$fitted.values
  y <- as.data.frame(cbind(x, x1))

  y <- y[order(x1), ]
  test <- setDT(y)[, as.list(colMeans(.SD)), by = gl(ceiling(nrow(y) / size), size, nrow(y))]
  test$res <- test$x - test$x1

  qplot(y = test$res, x = test$x1, ylab = "Residual", xlab = "Fitted Value", main = "Crunch Residual")
}

crunch_residual(
  dataset = AutoClaim,
  model = glm.freq,
  target = "CLM_FLAG_NUM",
  size = 50
)
```

```{r}
# CHUNK 10
# Deviance
crunch_residual2 <- function(dataset, model, target, size = 50) {
  r <- model$residuals
  x1 <- model$fitted.values
  y <- as.data.frame(cbind(r, x1))

  y <- y[order(x1), ]
  test <- setDT(y)[, as.list(colMeans(.SD)), by = gl(ceiling(nrow(y) / size), size, nrow(y))]

  qplot(y = test$r, x = test$x1, ylab = "Residual", xlab = "Fitted Value", main = "Crunch Residual")
}

crunch_residual2(
  dataset = AutoClaim,
  model = glm.freq,
  target = "CLM_FLAG_NUM",
  size = 50
)
```

CHUNK 11 provides some additional diagnostic plots.

```{r}
# CHUNK 11
# glm.diag.plots(glm.sev, glmdiag=glm.diag(glm.sev))
par(mfrow = c(2, 2))
plot(glm.sev)
```

Begin the example by running CHUNK 12 to create and plot a simple dataset.

```{r echo = FALSE}
# CHUNK 12
library(ggplot2)
df <- data.frame(x = seq(-1, 1, 0.3), y = c(2, 1.1, 0.7, 0.95, 0.4, -0.1, -0.05))

ggplot(data = df, aes(x = x, y = y)) +
  geom_point(color = "blue", size = 3)
```

Run CHUNK 13 to add the desired features to the dataframe.

```{r}
# CHUNK 13
df$X1 <- df$x
df$X2 <- df$x^2
df$X3 <- df$x^3
df$X4 <- df$x^4
df$X5 <- df$x^5
df$X6 <- df$x^6

df
```

Run CHUNK 14 to obtain the ordinary least squares fit, using the glmnet package.

```{r}
# CHUNK 14
library(glmnet)

# Place the X values in a matrix (required for the glmnet function).
X <- as.matrix(df[, 3:8])

# Set up the formula (model form).
formula.lm <- as.formula("y~X1+X2+X3+X4+X5+X6")

# Fit the model, lambda = 0 forces ordinary least squares and makes alpha irrelevant.
model.lm <- glmnet(X, y = df$y, family = "gaussian", alpha = 0, lambda = 0)

# Predict results (so we can plot the line).
df$pred <- predict(model.lm, newx = X)

# Plot the results.
p1 <- ggplot(data = df, aes(x = x, y = y)) +
  geom_point(color = "blue", size = 3) +
  geom_line(aes(y = pred))
p1
```

Run CHUNK 15 to view the estimated coefficients.

```{r}
# CHUNK 15
model.lm$a0
model.lm$beta
```

Run CHUNK 16 to perform ridge regression.

```{r}
# CHUNK 16
library(glmnet)

# Here is a clever way to create the data matrix. This one doesn't require keeping track of which columns contain the features.
X <- model.matrix(formula.lm, data = df)

# Lambda has arbitrarily been set to 0.1. Alpha = 0 implies ridge regression (1 implies lasso and anything between is elastic net). Also, note that the default is to standardize the features, but the estimated coefficients are on the scale and location of the original values.
model.lm.ridge <- glmnet(X,
  y = df$y,
  family = "gaussian",
  alpha = 0,
  lambda = 0.1
)

# Predict results (so we can plot the line)
df$pred_ridge01 <- predict(model.lm.ridge, newx = X)

# Plot the results
p1 <- ggplot(data = df, aes(x = x, y = y)) +
  geom_point(color = "blue", size = 3) +
  geom_line(aes(y = pred_ridge01))
p1
```

Run CHUNK 17 to see the coefficients.

```{r}
# CHUNK 17
model.lm.ridge$a0
model.lm.ridge$beta
```

The default for glmnet is to standardize the predictor variables. However, the coefficients presented above apply to the original values.

What happens when we change the value for lambda? Run CHUNK 18 to find out.

```{r}
# CHUNK 18
library(gridExtra)
model.lm.ridge00 <- glmnet(X, y = df$y, family = "gaussian", alpha = 0, lambda = 0)
model.lm.ridge01 <- glmnet(X, y = df$y, family = "gaussian", alpha = 0, lambda = 0.1)
model.lm.ridge05 <- glmnet(X, y = df$y, family = "gaussian", alpha = 0, lambda = 0.5)
model.lm.ridge1 <- glmnet(X, y = df$y, family = "gaussian", alpha = 0, lambda = 1)
model.lm.ridge10 <- glmnet(X, y = df$y, family = "gaussian", alpha = 0, lambda = 10)

df$pred_ridge00 <- predict(model.lm.ridge00, newx = X)
df$pred_ridge01 <- predict(model.lm.ridge01, newx = X)
df$pred_ridge05 <- predict(model.lm.ridge05, newx = X)
df$pred_ridge1 <- predict(model.lm.ridge1, newx = X)
df$pred_ridge10 <- predict(model.lm.ridge10, newx = X)

p1 <- ggplot(data = df, aes(x = x, y = y)) +
  geom_point(color = "blue", size = 3) +
  geom_line(aes(y = pred_ridge00)) +
  ggtitle("lambda = 0")
p2 <- ggplot(data = df, aes(x = x, y = y)) +
  geom_point(color = "blue", size = 3) +
  geom_line(aes(y = pred_ridge01)) +
  ggtitle("lambda = 0.1")
p3 <- ggplot(data = df, aes(x = x, y = y)) +
  geom_point(color = "blue", size = 3) +
  geom_line(aes(y = pred_ridge05)) +
  ggtitle("lambda = 0.5")
p4 <- ggplot(data = df, aes(x = x, y = y)) +
  geom_point(color = "blue", size = 3) +
  geom_line(aes(y = pred_ridge1)) +
  ggtitle("lambda = 1")
p5 <- ggplot(data = df, aes(x = x, y = y)) +
  geom_point(color = "blue", size = 3) +
  geom_line(aes(y = pred_ridge10)) +
  ggtitle("lambda = 10")

grid.arrange(p1, p2, ncol = 2)
grid.arrange(p3, p4, ncol = 2)
grid.arrange(p5, ncol = 2)
```

And the coefficients are produced in CHUNK 19.

```{r}
# CHUNK 19
model.lm.ridge00$a0
model.lm.ridge00$beta
model.lm.ridge01$a0
model.lm.ridge01$beta
model.lm.ridge05$a0
model.lm.ridge05$beta
model.lm.ridge1$a0
model.lm.ridge1$beta
model.lm.ridge10$a0
model.lm.ridge10$beta
```

CHUNK 20 is used to make an interesting graph, to explain the difference between ridge and lasso.

```{r echo = FALSE}
# CHUNK 20
x <- seq(-1.5, 1.5, 0.1)
penalties <- data.frame(
  beta = x,
  penalty = x^2,
  abs = abs(x)
)

ggplot(data = penalties, aes(x = beta)) +
  geom_line(aes(y = penalty), color = "red") +
  geom_line(aes(y = abs), color = "blue") +
  geom_line(aes(y = 1)) +
  annotate("rect", xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = 1, fill = "blue", alpha = 0.2) +
  annotate("rect", xmin = -Inf, xmax = Inf, ymin = 1, ymax = Inf, fill = "red", alpha = 0.2) +
  annotate("text", x = -1.4, y = 1.2, label = "Absolute value", color = "blue") +
  annotate("text", x = -1.1, y = 1.7, label = "Square", color = "red") +
  annotate("text", x = 0, y = 1.3, label = "Square penalty > Absolute value penalty", color = "black") +
  annotate("text", x = 0, y = 0.7, label = "Absolute value penalty > Square penalty", color = "black") +
  ggtitle("Absolute value penalty vs Square penalty")
```

CHUNK 21 performs lasso and ridge regressions on the seven-point data set.

```{r}
# CHUNK 21
library(glmnet)

X <- model.matrix(formula.lm, data = df)

model.lm.ridge <- glmnet(X, y = df$y, family = "gaussian", alpha = 0, lambda = 0.1)
model.lm.lasso <- glmnet(X, y = df$y, family = "gaussian", alpha = 1, lambda = 0.1)

# Predict results (so we can plot the line)
df$pred_ridge01 <- predict(model.lm.ridge, newx = X)
df$pred_lasso01 <- predict(model.lm.lasso, newx = X)

# Plot the results
p1 <- ggplot(data = df, aes(x = x, y = y)) +
  geom_point(color = "blue", size = 3) +
  geom_line(aes(y = pred_ridge01)) +
  ggtitle("Ridge - lambda = 0.1")
p2 <- ggplot(data = df, aes(x = x, y = y)) +
  geom_point(color = "blue", size = 3) +
  geom_line(aes(y = pred_lasso01)) +
  ggtitle("Lasso - lambda = 0.1")

grid.arrange(p1, p2, ncol = 2)
```

CHUNK 22 provides the coefficients. The intercepts are not displayed as the goal here is to see how the two approaches differ with regard to shrinking the non-intercept coefficients.

```{r}
# CHUNK 22
model.lm.ridge$beta
model.lm.lasso$beta
```

CHUNK 23 makes a picture that illustrates the various penalty functions.

```{r echo = FALSE}
# CHUNK 23
x <- seq(-1.5, 1.5, 0.1)
penalties <- data.frame(
  beta = x,
  penalty = x^2,
  abs = abs(x),
  el025 = 0.25 * abs(x) + 0.75 * x^2,
  el075 = 0.75 * abs(x) + 0.25 * x^2
)

ggplot(data = penalties, aes(x = beta)) +
  geom_line(aes(y = penalty), color = "red") +
  geom_line(aes(y = abs), color = "blue") +
  geom_line(aes(y = el025), color = "green") +
  geom_line(aes(y = el075), color = "purple") +
  geom_line(aes(y = 1)) +
  annotate("rect", xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = 1, fill = "blue", alpha = 0.2) +
  annotate("rect", xmin = -Inf, xmax = Inf, ymin = 1, ymax = Inf, fill = "red", alpha = 0.2) +
  annotate("text", x = -1.4, y = 1.2, label = "Absolute value", color = "blue") +
  annotate("text", x = -1.1, y = 1.7, label = "Square", color = "red") +
  annotate("text", x = 1.1, y = 1.7, label = "EN 0.25", color = "green") +
  annotate("text", x = 1.4, y = 1.2, label = "EN 0.75", color = "purple") +
  ggtitle("Absolute value penalty vs Square penalty vs Elastic Net")
```

CHUNK 24 runs elastic net regularization on the sample data.

```{r}
X <- model.matrix(formula.lm, data = df)
# CHUNK 24

model.lm.ridge <- glmnet(X, y = df$y, family = "gaussian", alpha = 0, lambda = 0.1)
model.lm.lasso <- glmnet(X, y = df$y, family = "gaussian", alpha = 1, lambda = 0.1)
model.lm.en025 <- glmnet(X, y = df$y, family = "gaussian", alpha = 0.25, lambda = 0.1)
model.lm.en075 <- glmnet(X, y = df$y, family = "gaussian", alpha = 0.75, lambda = 0.1)

# Predict results (so we can plot the line)
df$pred_ridge01 <- predict(model.lm.ridge, newx = X)
df$pred_lasso01 <- predict(model.lm.lasso, newx = X)
df$pred_en01025 <- predict(model.lm.en025, newx = X)
df$pred_en01075 <- predict(model.lm.en075, newx = X)

# Plot the results
p1 <- ggplot(data = df, aes(x = x, y = y)) +
  geom_point(color = "blue", size = 3) +
  geom_line(aes(y = pred_ridge01)) +
  ggtitle("Ridge - lambda = 0.1")
p2 <- ggplot(data = df, aes(x = x, y = y)) +
  geom_point(color = "blue", size = 3) +
  geom_line(aes(y = pred_lasso01)) +
  ggtitle("Lasso - lambda = 0.1")
p3 <- ggplot(data = df, aes(x = x, y = y)) +
  geom_point(color = "blue", size = 3) +
  geom_line(aes(y = pred_en01025)) +
  ggtitle("Elastic Net-lambda=0.1, alpha=0.25")
p4 <- ggplot(data = df, aes(x = x, y = y)) +
  geom_point(color = "blue", size = 3) +
  geom_line(aes(y = pred_en01075)) +
  ggtitle("Elastic Net-lambda=0.1, alpha=0.75")

grid.arrange(p1, p2, ncol = 2)
grid.arrange(p3, p4, ncol = 2)
```

CHUNK 25 displays the coefficients.

```{r}
# CHUNK 25
model.lm$beta
model.lm.ridge$beta
model.lm.lasso$beta
model.lm.en025$beta
model.lm.en075$beta
```
